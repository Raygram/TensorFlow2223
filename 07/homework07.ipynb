{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                            tf.keras.metrics.CategoricalAccuracy(name=\"acc\")]\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "        self.input_shape = (batch_size, time_steps, image)\n",
    "\n",
    "        self.layer_list = [\n",
    "\n",
    "                           tf.keras.layers.BatchNormalization(),\n",
    "                           tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', input_shape = self.input_shape[2:], activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', input_shape = self.input_shape[2:], activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', input_shape = self.input_shape[2:], activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', input_shape = self.input_shape[2:], activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', input_shape = self.input_shape[2:], activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', input_shape = self.input_shape[2:], activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)),\n",
    "                           \n",
    "                           tf.keras.layers.BatchNormalization(),\n",
    "                           tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', input_shape = self.input_shape[2:], activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', input_shape = self.input_shape[2:], activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', input_shape = self.input_shape[2:], activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', input_shape = self.input_shape[2:], activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.TimeDistributed(tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)),\n",
    "\n",
    "                           tf.keras.layers.BatchNormalization(),\n",
    "                           tf.keras.layers.Conv2D(filters=96, kernel_size=3, padding='same', input_shape = self.input_shape[2:], activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=96, kernel_size=3, padding='same', input_shape = self.input_shape[2:], activation='relu',kernel_regularizer=tf.keras.regularizers.L2(0.01)),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.TimeDistributed(tf.keras.layers.GlobalAveragePooling2D()),\n",
    "                           \n",
    "                           tf.keras.layers.Dense(10, activation='softmax')]\n",
    "                           \n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for item in self.layer_list:\n",
    "            x = item(x)\n",
    "        return x\n",
    "\n",
    "class RNNCell(tf.keras.layers.AbstractRNNCell):\n",
    "\n",
    "    def __init__(self, recurrent_units_1, recurrent_units_2, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "        self.recurrent_units_1 = recurrent_units_1\n",
    "        self.recurrent_units_2 = recurrent_units_2\n",
    "        \n",
    "        self.linear_1 = tf.keras.layers.Dense(recurrent_units_1)\n",
    "        self.linear_2 = tf.keras.layers.Dense(recurrent_units_2)\n",
    "        \n",
    "        # first recurrent layer in the RNN\n",
    "        self.recurrent_layer_1 = tf.keras.layers.Dense(recurrent_units_1, \n",
    "                                                       kernel_initializer= tf.keras.initializers.Orthogonal(\n",
    "                                                           gain=1.0, seed=None),\n",
    "                                                       activation=tf.nn.tanh)\n",
    "        # layer normalization for trainability\n",
    "        self.layer_norm_1 = tf.keras.layers.LayerNormalization()\n",
    "        \n",
    "        # second recurrent layer in the RNN\n",
    "        self.recurrent_layer_2 = tf.keras.layers.Dense(recurrent_units_2, \n",
    "                                                       kernel_initializer= tf.keras.initializers.Orthogonal(\n",
    "                                                           gain=1.0, seed=None), \n",
    "                                                       activation=tf.nn.tanh)\n",
    "        # layer normalization for trainability\n",
    "        self.layer_norm_2 = tf.keras.layers.LayerNormalization()\n",
    "    \n",
    "    @property\n",
    "    def state_size(self):\n",
    "        return [tf.TensorShape([self.recurrent_units_1]), \n",
    "                tf.TensorShape([self.recurrent_units_2])]\n",
    "    @property\n",
    "    def output_size(self):\n",
    "        return [tf.TensorShape([self.recurrent_units_2])]\n",
    "    \n",
    "    def get_initial_state(self, inputs=None, batch_size=None, dtype=None):\n",
    "        return [tf.zeros([self.recurrent_units_1]), \n",
    "                tf.zeros([self.recurrent_units_2])]\n",
    "\n",
    "    def call(self, inputs, states):\n",
    "        # unpack the states\n",
    "        state_layer_1 = states[0]\n",
    "        state_layer_2 = states[1]\n",
    "        \n",
    "        # linearly project input\n",
    "        x = self.linear_1(inputs) + state_layer_1\n",
    "        \n",
    "        # apply first recurrent kernel\n",
    "        new_state_layer_1 = self.recurrent_layer_1(x)\n",
    "        \n",
    "        # apply layer norm\n",
    "        x = self.layer_norm_1(new_state_layer_1)\n",
    "        \n",
    "        # linearly project output of layer norm\n",
    "        x = self.linear_2(x) + state_layer_2\n",
    "        \n",
    "        # apply second recurrent layer\n",
    "        new_state_layer_2 = self.recurrent_layer_2(x)\n",
    "        \n",
    "        # apply second layer's layer norm\n",
    "        x = self.layer_norm_2(new_state_layer_2)\n",
    "        \n",
    "        # return output and the list of new states of the layers\n",
    "        return x, [new_state_layer_1, new_state_layer_2]\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\"recurrent_units_1\": self.recurrent_units_1, \n",
    "                \"recurrent_units_2\": self.recurrent_units_2}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iannwtf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6 | packaged by conda-forge | (main, Oct 24 2022, 16:02:16) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63f83f16703f6be7ffd0b8723f3797201bb87b90d9af5c25669f8b533ad7062b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
