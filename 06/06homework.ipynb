{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import datetime\n",
    "import tqdm\n",
    "\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.random_saturation(image, 0.7, 1.3)\n",
    "    image = tf.image.random_contrast(image, 0.8, 1.2)\n",
    "    image = tf.image.random_brightness(image, 0.1)\n",
    "    return image, label \n",
    "\n",
    "def get_cifar10(batch_size):\n",
    "    \"\"\"\n",
    "    Load and prepare CIFAR-10 as a tensorflow dataset.\n",
    "    Returns a train and a validation dataset.\n",
    "    Args:\n",
    "    batch_size (int)\n",
    "    \"\"\"\n",
    "    train_ds, val_ds = tfds.load('cifar10', split=['train', 'test'], shuffle_files=True)\n",
    "\n",
    "    one_hot = lambda x: tf.one_hot(x, 10)\n",
    "\n",
    "    map_func = lambda x,y: (tf.cast(x, dtype=tf.float32)/255.,\n",
    "                            tf.cast(one_hot(y),tf.float32))\n",
    "\n",
    "    map_func_2 = lambda x: (x[\"image\"],x[\"label\"])\n",
    "\n",
    "    train_ds = train_ds.map(map_func_2).map(map_func).cache()\n",
    "    val_ds   = val_ds.map(map_func_2).map(map_func).cache()\n",
    "    \n",
    "    train_ds = train_ds.shuffle(4096).batch(batch_size)\n",
    "    val_ds   = val_ds.shuffle(4096).batch(batch_size)\n",
    "\n",
    "    return (train_ds.prefetch(tf.data.AUTOTUNE), val_ds.prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "train_ds, val_ds = get_cifar10(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNNBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, depth, layers):\n",
    "        super(BasicCNNBlock, self).__init__()\n",
    "        self.layers = [tf.keras.layers.Conv2D(filters=depth, kernel_size=3, padding='same', activation='relu') for _ in range(layers)]\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class BasicCNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                            tf.keras.metrics.CategoricalAccuracy(name=\"acc\")]\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "        self.layer_list = [\n",
    "\n",
    "                           tf.keras.layers.BatchNormalization(),\n",
    "                           tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', activation='relu'),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', activation='relu'),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', activation='relu'),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', activation='relu'),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', activation='relu'),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=24, kernel_size=3, padding='same', activation='relu'),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "                           \n",
    "                           tf.keras.layers.BatchNormalization(),\n",
    "                           tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu'),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu'),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu'),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=48, kernel_size=3, padding='same', activation='relu'),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.MaxPooling2D(pool_size=2, strides=2),\n",
    "\n",
    "                           tf.keras.layers.BatchNormalization(),\n",
    "                           tf.keras.layers.Conv2D(filters=96, kernel_size=3, padding='same', activation='relu'),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.Conv2D(filters=96, kernel_size=3, padding='same', activation='relu'),\n",
    "                           tf.keras.layers.Dropout(0.1),\n",
    "                           tf.keras.layers.GlobalAveragePooling2D(),\n",
    "                           \n",
    "                           tf.keras.layers.Dense(10, activation='softmax')]\n",
    "                           \n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for item in self.layer_list:\n",
    "            x = item(x)\n",
    "        return x\n",
    "     \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "        # return a list with all metrics in the model\n",
    "\n",
    "    # 4. reset all metrics objects\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "    # 5. train step method\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        img, target = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self(img, training=True)\n",
    "            loss = self.loss_function(target, output)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "        \n",
    "        # for all metrics except loss, update states (accuracy etc.)\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(target, output)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        img, target = data\n",
    "\n",
    "        output = self(img, training=False)\n",
    "        loss = self.loss_function(target, output)\n",
    "\n",
    "        self.metrics[0].update_state(loss)\n",
    "        # for accuracy metrics:\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(target, output)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where to save the log\n",
    "config_name= \"batchnormblock+dropout0.1layer+avg\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/train\"\n",
    "val_log_path = f\"logs/{config_name}/{current_time}/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "\n",
    "def training_loop(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        \n",
    "        # Training:\n",
    "        \n",
    "        for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
    "            metrics = model.train_step(data)\n",
    "            \n",
    "            # logging the validation metrics to the log file which is used by tensorboard\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in model.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        # print the metrics\n",
    "        print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # reset all metrics (requires a reset_metrics method in the model)\n",
    "        model.reset_metrics()    \n",
    "        \n",
    "        # Validation:\n",
    "        for data in val_ds:\n",
    "            metrics = model.test_step(data)\n",
    "        \n",
    "            # logging the validation metrics to the log file which is used by tensorboard\n",
    "            with val_summary_writer.as_default():\n",
    "                for metric in model.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "                    \n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # reset all metrics\n",
    "        model.reset_metrics()\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 14280), started 12 days, 22:52:20 ago. (Use '!kill 14280' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5c2f3c1fc53650f8\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5c2f3c1fc53650f8\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:45<00:00,  8.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 1.5343422889709473', 'acc: 0.42829999327659607']\n",
      "['val_loss: 1.2372716665267944', 'val_acc: 0.5515999794006348']\n",
      "\n",
      "\n",
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:41<00:00,  9.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 1.1181929111480713', 'acc: 0.598039984703064']\n",
      "['val_loss: 1.0342954397201538', 'val_acc: 0.6320000290870667']\n",
      "\n",
      "\n",
      "Epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:42<00:00,  9.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.9577516317367554', 'acc: 0.6566200256347656']\n",
      "['val_loss: 0.9239116907119751', 'val_acc: 0.671500027179718']\n",
      "\n",
      "\n",
      "Epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:41<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.8548796772956848', 'acc: 0.6969000101089478']\n",
      "['val_loss: 0.8497204184532166', 'val_acc: 0.7031999826431274']\n",
      "\n",
      "\n",
      "Epoch 4:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:41<00:00,  9.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.7824718952178955', 'acc: 0.7216399908065796']\n",
      "['val_loss: 0.7671656012535095', 'val_acc: 0.7314000129699707']\n",
      "\n",
      "\n",
      "Epoch 5:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:41<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.71678626537323', 'acc: 0.7467399835586548']\n",
      "['val_loss: 0.7466040253639221', 'val_acc: 0.7383999824523926']\n",
      "\n",
      "\n",
      "Epoch 6:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:41<00:00,  9.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.6592963933944702', 'acc: 0.7671999931335449']\n",
      "['val_loss: 0.7038795948028564', 'val_acc: 0.7526999711990356']\n",
      "\n",
      "\n",
      "Epoch 7:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:41<00:00,  9.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.6158493161201477', 'acc: 0.7825400233268738']\n",
      "['val_loss: 0.6779351234436035', 'val_acc: 0.7635999917984009']\n",
      "\n",
      "\n",
      "Epoch 8:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:41<00:00,  9.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.5873892903327942', 'acc: 0.7960799932479858']\n",
      "['val_loss: 0.6541395783424377', 'val_acc: 0.777400016784668']\n",
      "\n",
      "\n",
      "Epoch 9:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:41<00:00,  9.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.5541470646858215', 'acc: 0.8049600124359131']\n",
      "['val_loss: 0.6370955109596252', 'val_acc: 0.7857999801635742']\n",
      "\n",
      "\n",
      "Epoch 10:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:42<00:00,  9.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.5308927893638611', 'acc: 0.8144599795341492']\n",
      "['val_loss: 0.6137857437133789', 'val_acc: 0.7904999852180481']\n",
      "\n",
      "\n",
      "Epoch 11:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:41<00:00,  9.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.5046579241752625', 'acc: 0.8232399821281433']\n",
      "['val_loss: 0.5926381945610046', 'val_acc: 0.8009999990463257']\n",
      "\n",
      "\n",
      "Epoch 12:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:41<00:00,  9.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.4854346215724945', 'acc: 0.8290200233459473']\n",
      "['val_loss: 0.577062726020813', 'val_acc: 0.8011000156402588']\n",
      "\n",
      "\n",
      "Epoch 13:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:41<00:00,  9.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.4662816524505615', 'acc: 0.837440013885498']\n",
      "['val_loss: 0.5687354803085327', 'val_acc: 0.8068000078201294']\n",
      "\n",
      "\n",
      "Epoch 14:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:42<00:00,  9.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.452989399433136', 'acc: 0.841480016708374']\n",
      "['val_loss: 0.5821557641029358', 'val_acc: 0.8054999709129333']\n",
      "\n",
      "\n",
      "Epoch 15:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:40<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.433870404958725', 'acc: 0.8485400080680847']\n",
      "['val_loss: 0.5773168802261353', 'val_acc: 0.805899977684021']\n",
      "\n",
      "\n",
      "Epoch 16:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:39<00:00, 10.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.4241580069065094', 'acc: 0.8504800200462341']\n",
      "['val_loss: 0.5660180449485779', 'val_acc: 0.8033000230789185']\n",
      "\n",
      "\n",
      "Epoch 17:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:39<00:00,  9.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.40776288509368896', 'acc: 0.8557800054550171']\n",
      "['val_loss: 0.5499864220619202', 'val_acc: 0.8159999847412109']\n",
      "\n",
      "\n",
      "Epoch 18:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:39<00:00,  9.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.39452025294303894', 'acc: 0.8621000051498413']\n",
      "['val_loss: 0.5366323590278625', 'val_acc: 0.8203999996185303']\n",
      "\n",
      "\n",
      "Epoch 19:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [00:41<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 0.37911075353622437', 'acc: 0.8665599822998047']\n",
      "['val_loss: 0.5483778715133667', 'val_acc: 0.8192999958992004']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = BasicCNN()\n",
    "\n",
    "training_loop(model=model,\n",
    "                train_ds=train_ds, \n",
    "                val_ds=val_ds, \n",
    "                epochs=20, \n",
    "                train_summary_writer=train_summary_writer, \n",
    "                val_summary_writer=val_summary_writer)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this homework we tried many different setups regarding the optimization of last homeworks results. First we had a look at what difference each optimization technique alone had on model performance and in the end what combining these yielded on performance. So the basic network without any optimization has some overfitting, with training accuracy around 90% and validation accuracy around 70%. L2 regularization alone (we tried values 0.001; 0.01), very slightly improved model performance but not significantly. Then we tried different setups with dropout layers, one where we added dropout layers after each individual layer and in the second after each layer block and each with different dropout values (0.1 and 0.2). The model had the best performance with dropout layers after each individual layer with dropout value set to 0.1, it also helped with overfitting. Data augmentation alone had no significant impact on model performance. Lastly we added batch normalization before each layer block which also significantly improved performance and reduced overfitting. The best combination of techniques was batch norm before each layer block with dropout layers after each individual layer (value 0.1) and L2 regularization (value 0.001). Values and graphs for every experiment can be seen in the tensorboard :)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('iannwtf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "63f83f16703f6be7ffd0b8723f3797201bb87b90d9af5c25669f8b533ad7062b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
