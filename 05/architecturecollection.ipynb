{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import datetime\n",
    "import tqdm\n",
    "\n",
    "# in a notebook, load the tensorboard extension, not needed for scripts\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cifar10(batch_size):\n",
    "    \"\"\"\n",
    "    Load and prepare CIFAR-10 as a tensorflow dataset.\n",
    "    Returns a train and a validation dataset.\n",
    "    Args:\n",
    "    batch_size (int)\n",
    "    \"\"\"\n",
    "    train_ds, val_ds = tfds.load('cifar10', split=['train', 'test'], shuffle_files=True)\n",
    "\n",
    "    one_hot = lambda x: tf.one_hot(x, 10)\n",
    "\n",
    "    map_func = lambda x,y: (tf.cast(x, dtype=tf.float32)/255.,\n",
    "                            tf.cast(one_hot(y),tf.float32))\n",
    "\n",
    "    map_func_2 = lambda x: (x[\"image\"],x[\"label\"])\n",
    "\n",
    "    train_ds = train_ds.map(map_func_2).map(map_func).cache()\n",
    "    val_ds   = val_ds.map(map_func_2).map(map_func).cache()\n",
    "    \n",
    "    train_ds = train_ds.shuffle(4096).batch(batch_size)\n",
    "    val_ds   = val_ds.shuffle(4096).batch(batch_size)\n",
    "\n",
    "    return (train_ds.prefetch(tf.data.AUTOTUNE), val_ds.prefetch(tf.data.AUTOTUNE))\n",
    "\n",
    "train_ds, val_ds = get_cifar10(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenselyConnectedBottleneckCNNLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_filters, bottleneck_size):\n",
    "    super(DenselyConnectedBottleneckCNNLayer, self).__init__()\n",
    "    self.bottleneck = tf.keras.layers.Conv2D(filters=bottleneck_size, kernel_size=1, padding='same')\n",
    "    self.conv = tf.keras.layers.Conv2D(filters=num_filters, kernel_size=3, padding='same', activation='relu')\n",
    "\n",
    "  @tf.function\n",
    "  def call(self, x):\n",
    "    c = self.bottleneck(x)\n",
    "    c = self.conv(c)\n",
    "    x = tf.concat((x,c), axis=-1)\n",
    "    return x\n",
    "\n",
    "class DenselyConnectedBottleneckCNNBlock(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_filters, layers, bottleneck_size):\n",
    "    super(DenselyConnectedBottleneckCNNBlock, self).__init__()\n",
    "\n",
    "    self.layers = [DenselyConnectedBottleneckCNNLayer(num_filters, bottleneck_size) for _ in range(layers)]\n",
    "\n",
    "  @tf.function\n",
    "  def call(self, x):\n",
    "    for layer in self.layers:\n",
    "      x = layer(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class DenselyConnectedBottleneckCNN(tf.keras.Model):\n",
    "  def __init__(self):\n",
    "    super(DenselyConnectedBottleneckCNN, self).__init__()\n",
    "\n",
    "    self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                             tf.keras.metrics.CategoricalAccuracy(name=\"acc\")]\n",
    "        \n",
    "    self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "    self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "    self.denseblock1 = DenselyConnectedBottleneckCNNBlock(24,4, 24)\n",
    "    self.pooling1 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
    "\n",
    "    self.denseblock2 = DenselyConnectedBottleneckCNNBlock(24,4, 24)\n",
    "    self.pooling2 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
    "\n",
    "    self.denseblock3 = DenselyConnectedBottleneckCNNBlock(24,4, 24)\n",
    "    self.globalpooling = tf.keras.layers.GlobalAvgPool2D()\n",
    "    self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "\n",
    "  @tf.function\n",
    "  def call(self,x):\n",
    "    x = self.denseblock1(x)\n",
    "    x = self.pooling1(x)\n",
    "    x = self.denseblock2(x)\n",
    "    x = self.pooling2(x)\n",
    "    x = self.denseblock3(x)\n",
    "    x = self.globalpooling(x)\n",
    "    x = self.out(x)\n",
    "    return x\n",
    "\n",
    "  @property\n",
    "  def metrics(self):\n",
    "    return self.metrics_list\n",
    "        # return a list with all metrics in the model\n",
    "\n",
    "    # 4. reset all metrics objects\n",
    "  def reset_metrics(self):\n",
    "    for metric in self.metrics:\n",
    "      metric.reset_states()\n",
    "\n",
    "    # 5. train step method\n",
    "  @tf.function\n",
    "  def train_step(self, data):\n",
    "    img, target = data\n",
    "        \n",
    "    with tf.GradientTape() as tape:\n",
    "      output = self(img, training=True)\n",
    "      loss = self.loss_function(target, output)\n",
    "            \n",
    "      gradients = tape.gradient(loss, self.trainable_variables)\n",
    "      self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        # update loss metric\n",
    "      self.metrics[0].update_state(loss)\n",
    "        \n",
    "        # for all metrics except loss, update states (accuracy etc.)\n",
    "      for metric in self.metrics[1:]:\n",
    "        metric.update_state(target, output)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "    return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "  @tf.function\n",
    "  def test_step(self, data):\n",
    "    img, target = data\n",
    "\n",
    "    output = self(img, training=False)\n",
    "    loss = self.loss_function(target, output)\n",
    "\n",
    "    self.metrics[0].update_state(loss)\n",
    "        # for accuracy metrics:\n",
    "    for metric in self.metrics[1:]:\n",
    "      metric.update_state(target, output)\n",
    "\n",
    "    return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicCNNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_filters):\n",
    "        super(BasicCNNLayer, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters=num_filters, kernel_size=3, padding='same', activation='relu')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        return self.conv(x)\n",
    "\n",
    "class BasicCNNBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, depth, layers):\n",
    "        super(BasicCNNBlock, self).__init__()\n",
    "        self.layers = [BasicCNNLayer(depth) for _ in range(layers)]\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class BasicConv(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                            tf.keras.metrics.CategoricalAccuracy(name=\"acc\")]\n",
    "        \n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "        \n",
    "        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "        self.basicblock1 = BasicCNNBlock(24,2)\n",
    "        self.pooling1 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
    "\n",
    "        self.basicblock2 = BasicCNNBlock(48,3)\n",
    "        self.pooling2 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
    "\n",
    "        self.basicblock3 = BasicCNNBlock(96,4)\n",
    "        self.global_pool = tf.keras.layers.GlobalMaxPooling2D()\n",
    "\n",
    "        self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.basicblock1(x)\n",
    "        x = self.pooling1(x)\n",
    "        x = self.basicblock2(x)\n",
    "        x = self.pooling2(x)\n",
    "        x = self.basicblock3(x)\n",
    "        x = self.global_pool(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "     \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "        # return a list with all metrics in the model\n",
    "\n",
    "    # 4. reset all metrics objects\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "    # 5. train step method\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        img, target = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self(img, training=True)\n",
    "            loss = self.loss_function(target, output)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "        \n",
    "        # for all metrics except loss, update states (accuracy etc.)\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(target, output)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        img, target = data\n",
    "\n",
    "        output = self(img, training=False)\n",
    "        loss = self.loss_function(target, output)\n",
    "\n",
    "        self.metrics[0].update_state(loss)\n",
    "        # for accuracy metrics:\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(target, output)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualConnectedCNNLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_filters):\n",
    "        super(ResidualConnectedCNNLayer, self).__init__()\n",
    "        self.conv = tf.keras.layers.Conv2D(filters=num_filters, kernel_size=3, padding='same', activation='relu')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        c = self.conv(x)\n",
    "        x = c+x\n",
    "        return x\n",
    "\n",
    "class ResidualConnectedCNNBlock(tf.keras.layers.Layer):\n",
    "    def __init__(self, depth, layers):\n",
    "        super(ResidualConnectedCNNBlock, self).__init__()\n",
    "        self.deeper_layer = tf.keras.layers.Conv2D(filters=depth, kernel_size=3, padding='same', activation='relu')\n",
    "        self.layers = [ResidualConnectedCNNLayer(depth) for _ in range(layers)]\n",
    "\n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        x = self.deeper_layer(x)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ResidualConnectedCNN(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(ResidualConnectedCNN, self).__init__()\n",
    "\n",
    "        self.metrics_list = [tf.keras.metrics.Mean(name=\"loss\"),\n",
    "                            tf.keras.metrics.CategoricalAccuracy(name=\"acc\")]\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "        self.loss_function = tf.keras.losses.CategoricalCrossentropy()\n",
    "\n",
    "        self.residualblock1 = ResidualConnectedCNNBlock(32,4)\n",
    "        self.pooling1 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
    "\n",
    "        self.residualblock2 = ResidualConnectedCNNBlock(64,4)\n",
    "        self.pooling2 = tf.keras.layers.MaxPooling2D(pool_size=2, strides=2)\n",
    "\n",
    "        self.residualblock3 = ResidualConnectedCNNBlock(128,4)\n",
    "        self.globalpooling = tf.keras.layers.GlobalAvgPool2D()\n",
    "\n",
    "        self.out = tf.keras.layers.Dense(10, activation='softmax')\n",
    "\n",
    "    @tf.function\n",
    "    def call(self,x):\n",
    "        x = self.residualblock1(x)\n",
    "        x = self.pooling1(x)\n",
    "        x = self.residualblock2(x)\n",
    "        x = self.pooling2(x)\n",
    "        x = self.residualblock3(x)\n",
    "        x = self.globalpooling(x)\n",
    "        x = self.out(x)\n",
    "      \n",
    "        return x\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return self.metrics_list\n",
    "        # return a list with all metrics in the model\n",
    "\n",
    "    # 4. reset all metrics objects\n",
    "    def reset_metrics(self):\n",
    "        for metric in self.metrics:\n",
    "            metric.reset_states()\n",
    "\n",
    "    # 5. train step method\n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        img, target = data\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            output = self(img, training=True)\n",
    "            loss = self.loss_function(target, output)\n",
    "            \n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "\n",
    "        # update loss metric\n",
    "        self.metrics[0].update_state(loss)\n",
    "        \n",
    "        # for all metrics except loss, update states (accuracy etc.)\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(target, output)\n",
    "\n",
    "        # Return a dictionary mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "    @tf.function\n",
    "    def test_step(self, data):\n",
    "        img, target = data\n",
    "\n",
    "        output = self(img, training=False)\n",
    "        loss = self.loss_function(target, output)\n",
    "\n",
    "        self.metrics[0].update_state(loss)\n",
    "        # for accuracy metrics:\n",
    "        for metric in self.metrics[1:]:\n",
    "            metric.update_state(target, output)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where to save the log\n",
    "config_name= \"config_name\"\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "train_log_path = f\"logs/{config_name}/{current_time}/SGDwMomentum/train\"\n",
    "val_log_path = f\"logs/{config_name}/{current_time}/SGDwMomentum/val\"\n",
    "\n",
    "# log writer for training metrics\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_path)\n",
    "\n",
    "# log writer for validation metrics\n",
    "val_summary_writer = tf.summary.create_file_writer(val_log_path)\n",
    "\n",
    "def training_loop(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer):\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"Epoch {epoch}:\")\n",
    "        \n",
    "        # Training:\n",
    "        \n",
    "        for data in tqdm.tqdm(train_ds, position=0, leave=True):\n",
    "            metrics = model.train_step(data)\n",
    "            \n",
    "            # logging the validation metrics to the log file which is used by tensorboard\n",
    "            with train_summary_writer.as_default():\n",
    "                for metric in model.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "\n",
    "        # print the metrics\n",
    "        print([f\"{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # reset all metrics (requires a reset_metrics method in the model)\n",
    "        model.reset_metrics()    \n",
    "        \n",
    "        # Validation:\n",
    "        for data in val_ds:\n",
    "            metrics = model.test_step(data)\n",
    "        \n",
    "            # logging the validation metrics to the log file which is used by tensorboard\n",
    "            with val_summary_writer.as_default():\n",
    "                for metric in model.metrics:\n",
    "                    tf.summary.scalar(f\"{metric.name}\", metric.result(), step=epoch)\n",
    "                    \n",
    "        print([f\"val_{key}: {value.numpy()}\" for (key, value) in metrics.items()])\n",
    "\n",
    "        # reset all metrics\n",
    "        model.reset_metrics()\n",
    "        print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1968), started 0:20:50 ago. (Use '!kill 1968' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e1b1b385679879f0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e1b1b385679879f0\");\n",
       "          const url = new URL(\"http://localhost\");\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:42<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 1.762391209602356', 'acc: 0.34516000747680664']\n",
      "['val_loss: 1.4260337352752686', 'val_acc: 0.47110000252723694']\n",
      "\n",
      "\n",
      "Epoch 1:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:41<00:00,  3.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 1.3145231008529663', 'acc: 0.5218999981880188']\n",
      "['val_loss: 1.183062195777893', 'val_acc: 0.5651999711990356']\n",
      "\n",
      "\n",
      "Epoch 2:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 391/391 [01:43<00:00,  3.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['loss: 1.1050620079040527', 'acc: 0.6042199730873108']\n",
      "['val_loss: 1.0284942388534546', 'val_acc: 0.6355999708175659']\n",
      "\n",
      "\n",
      "Epoch 3:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 16/391 [00:04<01:46,  3.53it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [50], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m model \u001b[39m=\u001b[39m BasicConv()\n\u001b[1;32m----> 3\u001b[0m training_loop(model\u001b[39m=\u001b[39;49mmodel,\n\u001b[0;32m      4\u001b[0m                 train_ds\u001b[39m=\u001b[39;49mtrain_ds, \n\u001b[0;32m      5\u001b[0m                 val_ds\u001b[39m=\u001b[39;49mval_ds, \n\u001b[0;32m      6\u001b[0m                 epochs\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m, \n\u001b[0;32m      7\u001b[0m                 train_summary_writer\u001b[39m=\u001b[39;49mtrain_summary_writer, \n\u001b[0;32m      8\u001b[0m                 val_summary_writer\u001b[39m=\u001b[39;49mval_summary_writer)\n",
      "Cell \u001b[1;32mIn [46], line 24\u001b[0m, in \u001b[0;36mtraining_loop\u001b[1;34m(model, train_ds, val_ds, epochs, train_summary_writer, val_summary_writer)\u001b[0m\n\u001b[0;32m     21\u001b[0m metrics \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mtrain_step(data)\n\u001b[0;32m     23\u001b[0m \u001b[39m# logging the validation metrics to the log file which is used by tensorboard\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[39mwith\u001b[39;00m train_summary_writer\u001b[39m.\u001b[39mas_default():\n\u001b[0;32m     25\u001b[0m     \u001b[39mfor\u001b[39;00m metric \u001b[39min\u001b[39;00m model\u001b[39m.\u001b[39mmetrics:\n\u001b[0;32m     26\u001b[0m         tf\u001b[39m.\u001b[39msummary\u001b[39m.\u001b[39mscalar(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mmetric\u001b[39m.\u001b[39mname\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m, metric\u001b[39m.\u001b[39mresult(), step\u001b[39m=\u001b[39mepoch)\n",
      "File \u001b[1;32mc:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:91\u001b[0m, in \u001b[0;36m_SummaryContextManager.__exit__\u001b[1;34m(self, *exc)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__exit__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39mexc):\n\u001b[0;32m     89\u001b[0m   \u001b[39m# Flushes the summary writer in eager mode or in graph functions, but\u001b[39;00m\n\u001b[0;32m     90\u001b[0m   \u001b[39m# not in legacy graph mode (you're on your own there).\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m   _summary_state\u001b[39m.\u001b[39;49mwriter\u001b[39m.\u001b[39;49mflush()\n\u001b[0;32m     92\u001b[0m   _summary_state\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_old_writer\n\u001b[0;32m     93\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_step \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\tensorflow\\python\\ops\\summary_ops_v2.py:347\u001b[0m, in \u001b[0;36m_ResourceSummaryWriter.flush\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    345\u001b[0m   \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    346\u001b[0m \u001b[39mwith\u001b[39;00m ops\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu:0\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m--> 347\u001b[0m   \u001b[39mreturn\u001b[39;00m gen_summary_ops\u001b[39m.\u001b[39;49mflush_summary_writer(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_resource)\n",
      "File \u001b[1;32mc:\\Users\\benja\\Anaconda3\\envs\\iannwtf\\lib\\site-packages\\tensorflow\\python\\ops\\gen_summary_ops.py:193\u001b[0m, in \u001b[0;36mflush_summary_writer\u001b[1;34m(writer, name)\u001b[0m\n\u001b[0;32m    191\u001b[0m \u001b[39mif\u001b[39;00m tld\u001b[39m.\u001b[39mis_eager:\n\u001b[0;32m    192\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 193\u001b[0m     _result \u001b[39m=\u001b[39m pywrap_tfe\u001b[39m.\u001b[39;49mTFE_Py_FastPathExecute(\n\u001b[0;32m    194\u001b[0m       _ctx, \u001b[39m\"\u001b[39;49m\u001b[39mFlushSummaryWriter\u001b[39;49m\u001b[39m\"\u001b[39;49m, name, writer)\n\u001b[0;32m    195\u001b[0m     \u001b[39mreturn\u001b[39;00m _result\n\u001b[0;32m    196\u001b[0m   \u001b[39mexcept\u001b[39;00m _core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = BasicConv()\n",
    "\n",
    "training_loop(model=model,\n",
    "                train_ds=train_ds, \n",
    "                val_ds=val_ds, \n",
    "                epochs=15, \n",
    "                train_summary_writer=train_summary_writer, \n",
    "                val_summary_writer=val_summary_writer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('iannwtf')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5d1a1c06dd7fc6b133a3916b0943de0426aecca2a17bdc4d7dfa8117badf567a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
